Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job      count
-----  -------
all          1
ate          1
run          7
total        9

Select jobs to execute...
Execute 1 jobs...

[Sun May 18 01:55:22 2025]
localrule run:
    output: output/switch=1800/results.csv, output/switch=1800/config.csv
    jobid: 7
    reason: Missing output files: output/switch=1800/results.csv
    wildcards: switch_every=1800
    resources: tmpdir=/var/folders/84/sq9gqqh952qfzb3rpnrkcjqr0000gn/T

Will exit after finishing currently running jobs (scheduler).
[Sun May 18 01:57:18 2025]
Error in rule run:
    jobid: 7
    output: output/switch=1800/results.csv, output/switch=1800/config.csv
    shell:
        
        python rideshare-incremental.py with         switch_every=1800         output=output/switch=1800/results.csv         config_output=output/switch=1800/config.csv         n_events=50000         k=1        batch_size=1000         seed=42
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job run since they might be corrupted:
output/switch=1800/config.csv
Will exit after finishing currently running jobs (scheduler).
Shutting down, this might take some time.
Complete log: .snakemake/log/2025-05-18T015522.896327.snakemake.log
WorkflowError:
At least one job did not complete successfully.
