Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job      count
-----  -------
all          1
ate          1
run          7
total        9

Select jobs to execute...
Execute 1 jobs...

[Sun May 18 00:57:24 2025]
localrule run:
    output: output/switch=1800/results.csv, output/switch=1800/config.csv
    jobid: 7
    reason: Missing output files: output/switch=1800/results.csv
    wildcards: switch_every=1800
    resources: tmpdir=/var/folders/84/sq9gqqh952qfzb3rpnrkcjqr0000gn/T

[Sun May 18 00:57:25 2025]
Error in rule run:
    jobid: 7
    output: output/switch=1800/results.csv, output/switch=1800/config.csv
    shell:
        
        python rideshare-incremental.py with         switch_every=1800         output=output/switch=1800/results.csv         config_output=output/switch=1800/config.csv         n_events=500000         k=999        batch_size=1000         seed=42
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2025-05-18T005724.595722.snakemake.log
WorkflowError:
At least one job did not complete successfully.
